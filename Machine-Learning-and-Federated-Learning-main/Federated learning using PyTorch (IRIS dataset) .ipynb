{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fedarated learning using PyTorch (IRIS dataset).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPxVUX4ETb7CIaRgMTS3mtu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"4DMLM7oqq0CE"},"source":["import torch "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WgeB_Fcsq2sE","executionInfo":{"status":"ok","timestamp":1617857909071,"user_tz":-360,"elapsed":133663,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"87a3e663-cd57-41a2-b944-aff2f0f01185"},"source":["!pip install syft==0.2.5 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting syft==0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/db/8719b8859eb340ac7e21c03d358af894344d62edd429475e576bcb453480/syft-0.2.5-py3-none-any.whl (369kB)\n","\u001b[K     |████████████████████████████████| 378kB 11.3MB/s \n","\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.5) (1.0.2)\n","Collecting flask-socketio~=4.2.1\n","  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n","Collecting phe~=1.4.0\n","  Downloading https://files.pythonhosted.org/packages/32/0e/568e97b014eb14e794a1258a341361e9da351dc6240c63b89e1541e3341c/phe-1.4.0.tar.gz\n","Collecting tblib~=1.6.0\n","  Downloading https://files.pythonhosted.org/packages/0d/de/dca3e651ca62e59c08d324f4a51467fa4b8cbeaafb883b5e83720b4d4a47/tblib-1.6.0-py2.py3-none-any.whl\n","Collecting websockets~=8.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 5.0MB/s \n","\u001b[?25hCollecting numpy~=1.18.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n","\u001b[K     |████████████████████████████████| 20.1MB 68.4MB/s \n","\u001b[?25hCollecting Pillow~=6.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/3f/03375124676ab49ca6e6917c0f1f663afb8354d5d24e12f4fe4587a39ae2/Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 49.3MB/s \n","\u001b[?25hCollecting torchvision~=0.5.0\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.5.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (3.9MB)\n","\u001b[K     |████████████████████████████████| 3.9MB 14.6MB/s \n","\u001b[?25hRequirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.5) (1.1.2)\n","Collecting websocket-client~=0.57.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n","\u001b[K     |████████████████████████████████| 204kB 62.2MB/s \n","\u001b[?25hCollecting tornado==4.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/7b/e29ab3d51c8df66922fea216e2bddfcb6430fb29620e5165b16a216e0d3c/tornado-4.5.3.tar.gz (484kB)\n","\u001b[K     |████████████████████████████████| 491kB 41.1MB/s \n","\u001b[?25hCollecting torch~=1.4.0\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.4.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (640.6MB)\n","\u001b[K     |████████████████████████████████| 640.6MB 31kB/s \n","\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.5) (1.4.1)\n","Collecting requests~=2.22.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 4.0MB/s \n","\u001b[?25hCollecting lz4~=3.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/38/dacc3cbb33a9ded9e2e57f48707e8842f1080997901578ebddaa0e031646/lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 51.1MB/s \n","\u001b[?25hCollecting syft-proto~=0.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/32/88f77ae5e7f2d800c905a1bce147760b044132489343e4fd2c2506389a44/syft_proto-0.4.10-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n","\u001b[?25hCollecting python-socketio>=4.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/15/70ce203227a6c21931e0a1a04552887e8f08f7556459d65e901674f026fc/python_socketio-5.1.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision~=0.5.0->syft==0.2.5) (1.15.0)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.5) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.5) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.5) (7.1.2)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.1->syft==0.2.5) (2.11.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.5) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.5) (3.0.4)\n","Collecting idna<2.9,>=2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 3.9MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.22.0->syft==0.2.5) (2020.12.5)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from syft-proto~=0.4.1->syft==0.2.5) (3.12.4)\n","Collecting bidict>=0.21.0\n","  Downloading https://files.pythonhosted.org/packages/67/d4/eaf9242722bf991e0955380dd6168020cb15a71cc0d3cc2373f4911b1f1d/bidict-0.21.2-py2.py3-none-any.whl\n","Collecting python-engineio>=4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/ff/8c5392bcc6beb31aaeb759fce5de2141ae79a86018ad0173008f76b26085/python_engineio-4.0.1-py2.py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask~=1.1.1->syft==0.2.5) (1.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->syft-proto~=0.4.1->syft==0.2.5) (54.2.0)\n","Building wheels for collected packages: phe, tornado\n","  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=5f8710dc6b981833ae45f3187e67544d63cd06841d6ecd03c9c9fdc307c45dbe\n","  Stored in directory: /root/.cache/pip/wheels/f8/dc/36/dcb6bf0f1b9907e7b710ace63e64d08e7022340909315fdea4\n","  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=433997 sha256=1022afac60124908f17f35a7c056aee22edbdc7bbab1f0330d8fb0c71d769781\n","  Stored in directory: /root/.cache/pip/wheels/72/bf/f4/b68fa69596986881b397b18ff2b9af5f8181233aadcc9f76fd\n","Successfully built phe tornado\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0+cu92 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: bokeh 2.3.0 has requirement pillow>=7.1.0, but you'll have pillow 6.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: bokeh 2.3.0 has requirement tornado>=5.1, but you'll have tornado 4.5.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: bidict, python-engineio, python-socketio, flask-socketio, phe, tblib, websockets, numpy, Pillow, torch, torchvision, websocket-client, tornado, idna, requests, lz4, syft-proto, syft\n","  Found existing installation: tblib 1.7.0\n","    Uninstalling tblib-1.7.0:\n","      Successfully uninstalled tblib-1.7.0\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","  Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","Successfully installed Pillow-6.2.2 bidict-0.21.2 flask-socketio-4.2.1 idna-2.8 lz4-3.0.2 numpy-1.18.5 phe-1.4.0 python-engineio-4.0.1 python-socketio-5.1.0 requests-2.22.0 syft-0.2.5 syft-proto-0.4.10 tblib-1.6.0 torch-1.4.0+cu92 torchvision-0.5.0+cu92 tornado-4.5.3 websocket-client-0.57.0 websockets-8.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","numpy","torch","tornado"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"5K8eRer_q90Z"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNLJ8Qg1rrxU"},"source":["import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJzBk27ZsJHA"},"source":["dataset=load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5exjoXnsSxQ"},"source":["x=dataset.data\n","y=dataset.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxvaDeyRsb3o"},"source":["import numpy as np\n","x=x.astype(float)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nj9byPijsc4R","executionInfo":{"status":"ok","timestamp":1617685950567,"user_tz":-360,"elapsed":834,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"1be48d99-d44c-40ef-9d89-f3b1b244d1a5"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150, 4)"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"EmonxUDusfS4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617685206063,"user_tz":-360,"elapsed":845,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"1ea84c1b-859d-4640-be69-2cf142274f8b"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]])"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"NYfcxZCztMqY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617685654371,"user_tz":-360,"elapsed":931,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"878476aa-5d04-447c-8a5e-03a491a763ec"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"code","metadata":{"id":"urMCKXKEtNtR"},"source":["encoder= LabelEncoder()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sADzTgnxtYBZ"},"source":["y1=encoder.fit_transform(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1hf_SuUtypr"},"source":["y=pd.get_dummies(y1).values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6JM-_P1t0LY","executionInfo":{"status":"ok","timestamp":1617685216678,"user_tz":-360,"elapsed":821,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"f3c14d21-cf7d-462f-b136-392ec0540df1"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [1, 0, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 1, 0],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1],\n","       [0, 0, 1]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"dlNBBHxuunXC","executionInfo":{"status":"error","timestamp":1617637854235,"user_tz":-360,"elapsed":902,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"b1b971f5-b973-412f-da4e-f1576c33b233"},"source":["y = list(map(float, y))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-724196544839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"]}]},{"cell_type":"code","metadata":{"id":"wl7p8KORusgg"},"source":["from torch.utils.data import Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5eMn6DGwGSV"},"source":["class IRIS(Dataset):\n","  def __init__(self,images,labels,transform=None):\n","    self.data=images\n","    self.targets=labels\n","    self.to_torchtensor()\n","    self.transform = transform\n","  def to_torchtensor(self):\n","    self.data=torch.from_numpy(self.data).float()\n","    self.labels=torch.from_numpy(self.targets).float()\n","  def __len__(self):\n","    return len(self.data)\n","  def __getitem__(self, idx):\n","    sample=self.data[idx]\n","    target=self.targets[idx]\n","    if self.transform:\n","      sample = self.transform(sample)\n","    return sample,target \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3lKkL0JwJ_I"},"source":["import syft as sy\n","hook = sy.TorchHook(torch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HoTIkta3w6dG"},"source":["worker1 = sy.VirtualWorker(hook, id=\"worker1\")\n","worker2 = sy.VirtualWorker(hook, id=\"worker2\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWz-_7cQw_DI"},"source":["class Arguments():\n","    def __init__(self):\n","        self.batch_size = 10\n","        self.test_batch_size = 10\n","        self.epochs =50\n","        self.lr = 0.001\n","        self.momentum = 0.5\n","        self.no_cuda = False\n","        self.seed = 1\n","        self.log_interval = 5\n","        self.save_model = False\n","\n","args = Arguments()\n","\n","use_cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","torch.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcrQQ9yaxGZw"},"source":["\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMdj3YFPxzGo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617684828245,"user_tz":-360,"elapsed":978,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"42c635b8-9706-4507-f3f0-3812faf82f3d"},"source":["x_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[5.0000, 3.3000, 1.4000, 0.2000],\n","        [6.7000, 3.1000, 4.4000, 1.4000],\n","        [6.0000, 2.2000, 4.0000, 1.0000],\n","        [6.4000, 2.7000, 5.3000, 1.9000],\n","        [4.7000, 3.2000, 1.6000, 0.2000],\n","        [4.6000, 3.1000, 1.5000, 0.2000],\n","        [5.1000, 3.4000, 1.5000, 0.2000],\n","        [7.7000, 3.8000, 6.7000, 2.2000],\n","        [4.3000, 3.0000, 1.1000, 0.1000],\n","        [6.3000, 3.3000, 6.0000, 2.5000],\n","        [5.5000, 2.4000, 3.7000, 1.0000],\n","        [5.0000, 2.0000, 3.5000, 1.0000],\n","        [6.5000, 2.8000, 4.6000, 1.5000],\n","        [5.0000, 3.4000, 1.6000, 0.4000],\n","        [4.4000, 2.9000, 1.4000, 0.2000],\n","        [5.0000, 3.5000, 1.6000, 0.6000],\n","        [6.7000, 3.1000, 4.7000, 1.5000],\n","        [7.3000, 2.9000, 6.3000, 1.8000],\n","        [5.5000, 2.6000, 4.4000, 1.2000],\n","        [5.2000, 2.7000, 3.9000, 1.4000],\n","        [5.7000, 4.4000, 1.5000, 0.4000],\n","        [7.2000, 3.2000, 6.0000, 1.8000],\n","        [5.4000, 3.4000, 1.7000, 0.2000],\n","        [5.8000, 4.0000, 1.2000, 0.2000],\n","        [6.1000, 2.6000, 5.6000, 1.4000],\n","        [5.7000, 2.5000, 5.0000, 2.0000],\n","        [4.8000, 3.0000, 1.4000, 0.1000],\n","        [6.5000, 3.0000, 5.8000, 2.2000],\n","        [4.6000, 3.2000, 1.4000, 0.2000],\n","        [6.6000, 2.9000, 4.6000, 1.3000],\n","        [6.7000, 3.0000, 5.2000, 2.3000],\n","        [6.1000, 3.0000, 4.6000, 1.4000],\n","        [5.7000, 3.8000, 1.7000, 0.3000],\n","        [7.0000, 3.2000, 4.7000, 1.4000],\n","        [4.7000, 3.2000, 1.3000, 0.2000],\n","        [6.5000, 3.0000, 5.2000, 2.0000],\n","        [7.7000, 2.6000, 6.9000, 2.3000],\n","        [4.9000, 2.4000, 3.3000, 1.0000],\n","        [4.8000, 3.1000, 1.6000, 0.2000],\n","        [5.5000, 4.2000, 1.4000, 0.2000],\n","        [5.6000, 3.0000, 4.1000, 1.3000],\n","        [6.4000, 3.2000, 5.3000, 2.3000],\n","        [5.2000, 3.5000, 1.5000, 0.2000],\n","        [7.9000, 3.8000, 6.4000, 2.0000],\n","        [5.8000, 2.8000, 5.1000, 2.4000],\n","        [5.7000, 2.9000, 4.2000, 1.3000],\n","        [5.1000, 3.7000, 1.5000, 0.4000],\n","        [5.1000, 2.5000, 3.0000, 1.1000],\n","        [5.0000, 3.4000, 1.5000, 0.2000],\n","        [7.7000, 2.8000, 6.7000, 2.0000],\n","        [7.6000, 3.0000, 6.6000, 2.1000],\n","        [5.0000, 3.2000, 1.2000, 0.2000],\n","        [5.4000, 3.7000, 1.5000, 0.2000],\n","        [6.7000, 3.3000, 5.7000, 2.5000],\n","        [6.1000, 2.8000, 4.0000, 1.3000],\n","        [6.3000, 2.5000, 5.0000, 1.9000],\n","        [7.4000, 2.8000, 6.1000, 1.9000],\n","        [5.0000, 2.3000, 3.3000, 1.0000],\n","        [5.4000, 3.4000, 1.5000, 0.4000],\n","        [5.4000, 3.9000, 1.3000, 0.4000],\n","        [5.6000, 2.8000, 4.9000, 2.0000],\n","        [4.9000, 3.0000, 1.4000, 0.2000],\n","        [5.3000, 3.7000, 1.5000, 0.2000],\n","        [6.0000, 2.7000, 5.1000, 1.6000],\n","        [6.8000, 3.2000, 5.9000, 2.3000],\n","        [6.2000, 3.4000, 5.4000, 2.3000],\n","        [5.7000, 2.8000, 4.1000, 1.3000],\n","        [6.3000, 2.3000, 4.4000, 1.3000],\n","        [4.9000, 3.1000, 1.5000, 0.1000],\n","        [6.9000, 3.1000, 5.1000, 2.3000],\n","        [5.0000, 3.6000, 1.4000, 0.2000],\n","        [4.4000, 3.0000, 1.3000, 0.2000],\n","        [6.0000, 2.9000, 4.5000, 1.5000],\n","        [6.1000, 2.8000, 4.7000, 1.2000],\n","        [6.4000, 2.8000, 5.6000, 2.1000],\n","        [4.9000, 3.1000, 1.5000, 0.2000],\n","        [5.6000, 2.9000, 3.6000, 1.3000],\n","        [5.9000, 3.0000, 4.2000, 1.5000],\n","        [6.3000, 2.7000, 4.9000, 1.8000],\n","        [6.8000, 3.0000, 5.5000, 2.1000],\n","        [5.5000, 2.3000, 4.0000, 1.3000],\n","        [6.3000, 2.8000, 5.1000, 1.5000],\n","        [4.8000, 3.4000, 1.9000, 0.2000],\n","        [6.3000, 3.3000, 4.7000, 1.6000],\n","        [5.6000, 2.5000, 3.9000, 1.1000],\n","        [5.1000, 3.8000, 1.9000, 0.4000],\n","        [5.1000, 3.8000, 1.5000, 0.3000],\n","        [4.6000, 3.4000, 1.4000, 0.3000],\n","        [5.7000, 2.8000, 4.5000, 1.3000],\n","        [6.6000, 3.0000, 4.4000, 1.4000],\n","        [5.1000, 3.5000, 1.4000, 0.2000],\n","        [7.7000, 3.0000, 6.1000, 2.3000],\n","        [6.1000, 3.0000, 4.9000, 1.8000],\n","        [6.2000, 2.2000, 4.5000, 1.5000],\n","        [6.3000, 3.4000, 5.6000, 2.4000],\n","        [4.4000, 3.2000, 1.3000, 0.2000],\n","        [6.5000, 3.2000, 5.1000, 2.0000],\n","        [5.5000, 2.5000, 4.0000, 1.3000],\n","        [6.3000, 2.5000, 4.9000, 1.5000],\n","        [5.1000, 3.3000, 1.7000, 0.5000],\n","        [5.8000, 2.7000, 5.1000, 1.9000],\n","        [5.8000, 2.6000, 4.0000, 1.2000],\n","        [6.3000, 2.9000, 5.6000, 1.8000],\n","        [5.8000, 2.7000, 4.1000, 1.0000],\n","        [5.0000, 3.0000, 1.6000, 0.2000]])"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"722NU5Rm2f2q"},"source":["\n","federated_train_loader = sy.FederatedDataLoader(\n","                         IRIS(x_train,y_train)\n","                         .federate((worker1, worker2)),batch_size=args.batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybHlqhY12svw"},"source":["test_loader=torch.utils.data.DataLoader(IRIS(x_test,y_test), batch_size=args.batch_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zziazLLaUWBw"},"source":["class Net(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.fc1 = nn.Linear(in_features=4, out_features=16)\n","    self.fc2 = nn.Linear(in_features=16, out_features=12)\n","    self.output = nn.Linear(in_features=12, out_features=3)\n","  def forward(self, x):\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.output(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWgXUT5pUlrb"},"source":["import numpy as np\n","def train( args, model, device, train_loader, optimizer, epoch):\n","    train_loss=[]\n","    model.train()\n","    loss=0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        model.send(data.location)\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss= F.nll_loss(output, target).get()\n","        print(\"Epoch:\", epoch, \"Training Loss: \", loss.item())\n","        train_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        model.get()\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPaHId44UtdU"},"source":["def test( model, device, test_loader):\n","    test_loss=[]\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss =F.nll_loss(output, target)\n","            print(\"Epoch:\", epoch, \"Test Loss: \", loss.item())\n","           \n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDjT4oaeUxtL","executionInfo":{"status":"ok","timestamp":1617859400014,"user_tz":-360,"elapsed":19819,"user":{"displayName":"Nazmul Hossen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisO2NrkW2lgGRHkf-xhEd4t0XgVbkpgqSyWckVQw=s64","userId":"15244340077921146002"}},"outputId":"ff270f6a-b182-4933-b8e8-d2d4cab2207b"},"source":["model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=args.lr) \n","for epoch in range(1, args.epochs + 1):\n","    \n","    train(args, model, device, federated_train_loader, optimizer, epoch)\n","    test(model,device,test_loader)\n","   \n","    \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1 Training Loss:  0.3922111392021179\n","Epoch: 1 Training Loss:  0.5148643255233765\n","Epoch: 1 Training Loss:  0.362373411655426\n","Epoch: 1 Training Loss:  0.42786845564842224\n","Epoch: 1 Training Loss:  0.4195229113101959\n","Epoch: 1 Training Loss:  0.34590497612953186\n","Epoch: 1 Training Loss:  0.38483625650405884\n","Epoch: 1 Training Loss:  0.45342594385147095\n","Epoch: 1 Training Loss:  0.45700564980506897\n","Epoch: 1 Training Loss:  0.45152005553245544\n","Epoch: 1 Training Loss:  0.4558667540550232\n","Epoch: 1 Training Loss:  0.4727829694747925\n","Epoch: 1 Test Loss:  0.4807301461696625\n","Epoch: 1 Test Loss:  0.42843955755233765\n","Epoch: 1 Test Loss:  0.46474510431289673\n","Epoch: 1 Test Loss:  0.37971895933151245\n","Epoch: 1 Test Loss:  0.47564181685447693\n","Epoch: 2 Training Loss:  0.3922111392021179\n","Epoch: 2 Training Loss:  0.5148643255233765\n","Epoch: 2 Training Loss:  0.362373411655426\n","Epoch: 2 Training Loss:  0.42786845564842224\n","Epoch: 2 Training Loss:  0.4195229113101959\n","Epoch: 2 Training Loss:  0.34590497612953186\n","Epoch: 2 Training Loss:  0.38483625650405884\n","Epoch: 2 Training Loss:  0.45342594385147095\n","Epoch: 2 Training Loss:  0.45700564980506897\n","Epoch: 2 Training Loss:  0.45152005553245544\n","Epoch: 2 Training Loss:  0.4558667540550232\n","Epoch: 2 Training Loss:  0.4727829694747925\n","Epoch: 2 Test Loss:  0.4807301461696625\n","Epoch: 2 Test Loss:  0.42843955755233765\n","Epoch: 2 Test Loss:  0.46474510431289673\n","Epoch: 2 Test Loss:  0.37971895933151245\n","Epoch: 2 Test Loss:  0.47564181685447693\n","Epoch: 3 Training Loss:  0.3922111392021179\n","Epoch: 3 Training Loss:  0.5148643255233765\n","Epoch: 3 Training Loss:  0.362373411655426\n","Epoch: 3 Training Loss:  0.42786845564842224\n","Epoch: 3 Training Loss:  0.4195229113101959\n","Epoch: 3 Training Loss:  0.34590497612953186\n","Epoch: 3 Training Loss:  0.38483625650405884\n","Epoch: 3 Training Loss:  0.45342594385147095\n","Epoch: 3 Training Loss:  0.45700564980506897\n","Epoch: 3 Training Loss:  0.45152005553245544\n","Epoch: 3 Training Loss:  0.4558667540550232\n","Epoch: 3 Training Loss:  0.4727829694747925\n","Epoch: 3 Test Loss:  0.4807301461696625\n","Epoch: 3 Test Loss:  0.42843955755233765\n","Epoch: 3 Test Loss:  0.46474510431289673\n","Epoch: 3 Test Loss:  0.37971895933151245\n","Epoch: 3 Test Loss:  0.47564181685447693\n","Epoch: 4 Training Loss:  0.3922111392021179\n","Epoch: 4 Training Loss:  0.5148643255233765\n","Epoch: 4 Training Loss:  0.362373411655426\n","Epoch: 4 Training Loss:  0.42786845564842224\n","Epoch: 4 Training Loss:  0.4195229113101959\n","Epoch: 4 Training Loss:  0.34590497612953186\n","Epoch: 4 Training Loss:  0.38483625650405884\n","Epoch: 4 Training Loss:  0.45342594385147095\n","Epoch: 4 Training Loss:  0.45700564980506897\n","Epoch: 4 Training Loss:  0.45152005553245544\n","Epoch: 4 Training Loss:  0.4558667540550232\n","Epoch: 4 Training Loss:  0.4727829694747925\n","Epoch: 4 Test Loss:  0.4807301461696625\n","Epoch: 4 Test Loss:  0.42843955755233765\n","Epoch: 4 Test Loss:  0.46474510431289673\n","Epoch: 4 Test Loss:  0.37971895933151245\n","Epoch: 4 Test Loss:  0.47564181685447693\n","Epoch: 5 Training Loss:  0.3922111392021179\n","Epoch: 5 Training Loss:  0.5148643255233765\n","Epoch: 5 Training Loss:  0.362373411655426\n","Epoch: 5 Training Loss:  0.42786845564842224\n","Epoch: 5 Training Loss:  0.4195229113101959\n","Epoch: 5 Training Loss:  0.34590497612953186\n","Epoch: 5 Training Loss:  0.38483625650405884\n","Epoch: 5 Training Loss:  0.45342594385147095\n","Epoch: 5 Training Loss:  0.45700564980506897\n","Epoch: 5 Training Loss:  0.45152005553245544\n","Epoch: 5 Training Loss:  0.4558667540550232\n","Epoch: 5 Training Loss:  0.4727829694747925\n","Epoch: 5 Test Loss:  0.4807301461696625\n","Epoch: 5 Test Loss:  0.42843955755233765\n","Epoch: 5 Test Loss:  0.46474510431289673\n","Epoch: 5 Test Loss:  0.37971895933151245\n","Epoch: 5 Test Loss:  0.47564181685447693\n","Epoch: 6 Training Loss:  0.3922111392021179\n","Epoch: 6 Training Loss:  0.5148643255233765\n","Epoch: 6 Training Loss:  0.362373411655426\n","Epoch: 6 Training Loss:  0.42786845564842224\n","Epoch: 6 Training Loss:  0.4195229113101959\n","Epoch: 6 Training Loss:  0.34590497612953186\n","Epoch: 6 Training Loss:  0.38483625650405884\n","Epoch: 6 Training Loss:  0.45342594385147095\n","Epoch: 6 Training Loss:  0.45700564980506897\n","Epoch: 6 Training Loss:  0.45152005553245544\n","Epoch: 6 Training Loss:  0.4558667540550232\n","Epoch: 6 Training Loss:  0.4727829694747925\n","Epoch: 6 Test Loss:  0.4807301461696625\n","Epoch: 6 Test Loss:  0.42843955755233765\n","Epoch: 6 Test Loss:  0.46474510431289673\n","Epoch: 6 Test Loss:  0.37971895933151245\n","Epoch: 6 Test Loss:  0.47564181685447693\n","Epoch: 7 Training Loss:  0.3922111392021179\n","Epoch: 7 Training Loss:  0.5148643255233765\n","Epoch: 7 Training Loss:  0.362373411655426\n","Epoch: 7 Training Loss:  0.42786845564842224\n","Epoch: 7 Training Loss:  0.4195229113101959\n","Epoch: 7 Training Loss:  0.34590497612953186\n","Epoch: 7 Training Loss:  0.38483625650405884\n","Epoch: 7 Training Loss:  0.45342594385147095\n","Epoch: 7 Training Loss:  0.45700564980506897\n","Epoch: 7 Training Loss:  0.45152005553245544\n","Epoch: 7 Training Loss:  0.4558667540550232\n","Epoch: 7 Training Loss:  0.4727829694747925\n","Epoch: 7 Test Loss:  0.4807301461696625\n","Epoch: 7 Test Loss:  0.42843955755233765\n","Epoch: 7 Test Loss:  0.46474510431289673\n","Epoch: 7 Test Loss:  0.37971895933151245\n","Epoch: 7 Test Loss:  0.47564181685447693\n","Epoch: 8 Training Loss:  0.3922111392021179\n","Epoch: 8 Training Loss:  0.5148643255233765\n","Epoch: 8 Training Loss:  0.362373411655426\n","Epoch: 8 Training Loss:  0.42786845564842224\n","Epoch: 8 Training Loss:  0.4195229113101959\n","Epoch: 8 Training Loss:  0.34590497612953186\n","Epoch: 8 Training Loss:  0.38483625650405884\n","Epoch: 8 Training Loss:  0.45342594385147095\n","Epoch: 8 Training Loss:  0.45700564980506897\n","Epoch: 8 Training Loss:  0.45152005553245544\n","Epoch: 8 Training Loss:  0.4558667540550232\n","Epoch: 8 Training Loss:  0.4727829694747925\n","Epoch: 8 Test Loss:  0.4807301461696625\n","Epoch: 8 Test Loss:  0.42843955755233765\n","Epoch: 8 Test Loss:  0.46474510431289673\n","Epoch: 8 Test Loss:  0.37971895933151245\n","Epoch: 8 Test Loss:  0.47564181685447693\n","Epoch: 9 Training Loss:  0.3922111392021179\n","Epoch: 9 Training Loss:  0.5148643255233765\n","Epoch: 9 Training Loss:  0.362373411655426\n","Epoch: 9 Training Loss:  0.42786845564842224\n","Epoch: 9 Training Loss:  0.4195229113101959\n","Epoch: 9 Training Loss:  0.34590497612953186\n","Epoch: 9 Training Loss:  0.38483625650405884\n","Epoch: 9 Training Loss:  0.45342594385147095\n","Epoch: 9 Training Loss:  0.45700564980506897\n","Epoch: 9 Training Loss:  0.45152005553245544\n","Epoch: 9 Training Loss:  0.4558667540550232\n","Epoch: 9 Training Loss:  0.4727829694747925\n","Epoch: 9 Test Loss:  0.4807301461696625\n","Epoch: 9 Test Loss:  0.42843955755233765\n","Epoch: 9 Test Loss:  0.46474510431289673\n","Epoch: 9 Test Loss:  0.37971895933151245\n","Epoch: 9 Test Loss:  0.47564181685447693\n","Epoch: 10 Training Loss:  0.3922111392021179\n","Epoch: 10 Training Loss:  0.5148643255233765\n","Epoch: 10 Training Loss:  0.362373411655426\n","Epoch: 10 Training Loss:  0.42786845564842224\n","Epoch: 10 Training Loss:  0.4195229113101959\n","Epoch: 10 Training Loss:  0.34590497612953186\n","Epoch: 10 Training Loss:  0.38483625650405884\n","Epoch: 10 Training Loss:  0.45342594385147095\n","Epoch: 10 Training Loss:  0.45700564980506897\n","Epoch: 10 Training Loss:  0.45152005553245544\n","Epoch: 10 Training Loss:  0.4558667540550232\n","Epoch: 10 Training Loss:  0.4727829694747925\n","Epoch: 10 Test Loss:  0.4807301461696625\n","Epoch: 10 Test Loss:  0.42843955755233765\n","Epoch: 10 Test Loss:  0.46474510431289673\n","Epoch: 10 Test Loss:  0.37971895933151245\n","Epoch: 10 Test Loss:  0.47564181685447693\n","Epoch: 11 Training Loss:  0.3922111392021179\n","Epoch: 11 Training Loss:  0.5148643255233765\n","Epoch: 11 Training Loss:  0.362373411655426\n","Epoch: 11 Training Loss:  0.42786845564842224\n","Epoch: 11 Training Loss:  0.4195229113101959\n","Epoch: 11 Training Loss:  0.34590497612953186\n","Epoch: 11 Training Loss:  0.38483625650405884\n","Epoch: 11 Training Loss:  0.45342594385147095\n","Epoch: 11 Training Loss:  0.45700564980506897\n","Epoch: 11 Training Loss:  0.45152005553245544\n","Epoch: 11 Training Loss:  0.4558667540550232\n","Epoch: 11 Training Loss:  0.4727829694747925\n","Epoch: 11 Test Loss:  0.4807301461696625\n","Epoch: 11 Test Loss:  0.42843955755233765\n","Epoch: 11 Test Loss:  0.46474510431289673\n","Epoch: 11 Test Loss:  0.37971895933151245\n","Epoch: 11 Test Loss:  0.47564181685447693\n","Epoch: 12 Training Loss:  0.3922111392021179\n","Epoch: 12 Training Loss:  0.5148643255233765\n","Epoch: 12 Training Loss:  0.362373411655426\n","Epoch: 12 Training Loss:  0.42786845564842224\n","Epoch: 12 Training Loss:  0.4195229113101959\n","Epoch: 12 Training Loss:  0.34590497612953186\n","Epoch: 12 Training Loss:  0.38483625650405884\n","Epoch: 12 Training Loss:  0.45342594385147095\n","Epoch: 12 Training Loss:  0.45700564980506897\n","Epoch: 12 Training Loss:  0.45152005553245544\n","Epoch: 12 Training Loss:  0.4558667540550232\n","Epoch: 12 Training Loss:  0.4727829694747925\n","Epoch: 12 Test Loss:  0.4807301461696625\n","Epoch: 12 Test Loss:  0.42843955755233765\n","Epoch: 12 Test Loss:  0.46474510431289673\n","Epoch: 12 Test Loss:  0.37971895933151245\n","Epoch: 12 Test Loss:  0.47564181685447693\n","Epoch: 13 Training Loss:  0.3922111392021179\n","Epoch: 13 Training Loss:  0.5148643255233765\n","Epoch: 13 Training Loss:  0.362373411655426\n","Epoch: 13 Training Loss:  0.42786845564842224\n","Epoch: 13 Training Loss:  0.4195229113101959\n","Epoch: 13 Training Loss:  0.34590497612953186\n","Epoch: 13 Training Loss:  0.38483625650405884\n","Epoch: 13 Training Loss:  0.45342594385147095\n","Epoch: 13 Training Loss:  0.45700564980506897\n","Epoch: 13 Training Loss:  0.45152005553245544\n","Epoch: 13 Training Loss:  0.4558667540550232\n","Epoch: 13 Training Loss:  0.4727829694747925\n","Epoch: 13 Test Loss:  0.4807301461696625\n","Epoch: 13 Test Loss:  0.42843955755233765\n","Epoch: 13 Test Loss:  0.46474510431289673\n","Epoch: 13 Test Loss:  0.37971895933151245\n","Epoch: 13 Test Loss:  0.47564181685447693\n","Epoch: 14 Training Loss:  0.3922111392021179\n","Epoch: 14 Training Loss:  0.5148643255233765\n","Epoch: 14 Training Loss:  0.362373411655426\n","Epoch: 14 Training Loss:  0.42786845564842224\n","Epoch: 14 Training Loss:  0.4195229113101959\n","Epoch: 14 Training Loss:  0.34590497612953186\n","Epoch: 14 Training Loss:  0.38483625650405884\n","Epoch: 14 Training Loss:  0.45342594385147095\n","Epoch: 14 Training Loss:  0.45700564980506897\n","Epoch: 14 Training Loss:  0.45152005553245544\n","Epoch: 14 Training Loss:  0.4558667540550232\n","Epoch: 14 Training Loss:  0.4727829694747925\n","Epoch: 14 Test Loss:  0.4807301461696625\n","Epoch: 14 Test Loss:  0.42843955755233765\n","Epoch: 14 Test Loss:  0.46474510431289673\n","Epoch: 14 Test Loss:  0.37971895933151245\n","Epoch: 14 Test Loss:  0.47564181685447693\n","Epoch: 15 Training Loss:  0.3922111392021179\n","Epoch: 15 Training Loss:  0.5148643255233765\n","Epoch: 15 Training Loss:  0.362373411655426\n","Epoch: 15 Training Loss:  0.42786845564842224\n","Epoch: 15 Training Loss:  0.4195229113101959\n","Epoch: 15 Training Loss:  0.34590497612953186\n","Epoch: 15 Training Loss:  0.38483625650405884\n","Epoch: 15 Training Loss:  0.45342594385147095\n","Epoch: 15 Training Loss:  0.45700564980506897\n","Epoch: 15 Training Loss:  0.45152005553245544\n","Epoch: 15 Training Loss:  0.4558667540550232\n","Epoch: 15 Training Loss:  0.4727829694747925\n","Epoch: 15 Test Loss:  0.4807301461696625\n","Epoch: 15 Test Loss:  0.42843955755233765\n","Epoch: 15 Test Loss:  0.46474510431289673\n","Epoch: 15 Test Loss:  0.37971895933151245\n","Epoch: 15 Test Loss:  0.47564181685447693\n","Epoch: 16 Training Loss:  0.3922111392021179\n","Epoch: 16 Training Loss:  0.5148643255233765\n","Epoch: 16 Training Loss:  0.362373411655426\n","Epoch: 16 Training Loss:  0.42786845564842224\n","Epoch: 16 Training Loss:  0.4195229113101959\n","Epoch: 16 Training Loss:  0.34590497612953186\n","Epoch: 16 Training Loss:  0.38483625650405884\n","Epoch: 16 Training Loss:  0.45342594385147095\n","Epoch: 16 Training Loss:  0.45700564980506897\n","Epoch: 16 Training Loss:  0.45152005553245544\n","Epoch: 16 Training Loss:  0.4558667540550232\n","Epoch: 16 Training Loss:  0.4727829694747925\n","Epoch: 16 Test Loss:  0.4807301461696625\n","Epoch: 16 Test Loss:  0.42843955755233765\n","Epoch: 16 Test Loss:  0.46474510431289673\n","Epoch: 16 Test Loss:  0.37971895933151245\n","Epoch: 16 Test Loss:  0.47564181685447693\n","Epoch: 17 Training Loss:  0.3922111392021179\n","Epoch: 17 Training Loss:  0.5148643255233765\n","Epoch: 17 Training Loss:  0.362373411655426\n","Epoch: 17 Training Loss:  0.42786845564842224\n","Epoch: 17 Training Loss:  0.4195229113101959\n","Epoch: 17 Training Loss:  0.34590497612953186\n","Epoch: 17 Training Loss:  0.38483625650405884\n","Epoch: 17 Training Loss:  0.45342594385147095\n","Epoch: 17 Training Loss:  0.45700564980506897\n","Epoch: 17 Training Loss:  0.45152005553245544\n","Epoch: 17 Training Loss:  0.4558667540550232\n","Epoch: 17 Training Loss:  0.4727829694747925\n","Epoch: 17 Test Loss:  0.4807301461696625\n","Epoch: 17 Test Loss:  0.42843955755233765\n","Epoch: 17 Test Loss:  0.46474510431289673\n","Epoch: 17 Test Loss:  0.37971895933151245\n","Epoch: 17 Test Loss:  0.47564181685447693\n","Epoch: 18 Training Loss:  0.3922111392021179\n","Epoch: 18 Training Loss:  0.5148643255233765\n","Epoch: 18 Training Loss:  0.362373411655426\n","Epoch: 18 Training Loss:  0.42786845564842224\n","Epoch: 18 Training Loss:  0.4195229113101959\n","Epoch: 18 Training Loss:  0.34590497612953186\n","Epoch: 18 Training Loss:  0.38483625650405884\n","Epoch: 18 Training Loss:  0.45342594385147095\n","Epoch: 18 Training Loss:  0.45700564980506897\n","Epoch: 18 Training Loss:  0.45152005553245544\n","Epoch: 18 Training Loss:  0.4558667540550232\n","Epoch: 18 Training Loss:  0.4727829694747925\n","Epoch: 18 Test Loss:  0.4807301461696625\n","Epoch: 18 Test Loss:  0.42843955755233765\n","Epoch: 18 Test Loss:  0.46474510431289673\n","Epoch: 18 Test Loss:  0.37971895933151245\n","Epoch: 18 Test Loss:  0.47564181685447693\n","Epoch: 19 Training Loss:  0.3922111392021179\n","Epoch: 19 Training Loss:  0.5148643255233765\n","Epoch: 19 Training Loss:  0.362373411655426\n","Epoch: 19 Training Loss:  0.42786845564842224\n","Epoch: 19 Training Loss:  0.4195229113101959\n","Epoch: 19 Training Loss:  0.34590497612953186\n","Epoch: 19 Training Loss:  0.38483625650405884\n","Epoch: 19 Training Loss:  0.45342594385147095\n","Epoch: 19 Training Loss:  0.45700564980506897\n","Epoch: 19 Training Loss:  0.45152005553245544\n","Epoch: 19 Training Loss:  0.4558667540550232\n","Epoch: 19 Training Loss:  0.4727829694747925\n","Epoch: 19 Test Loss:  0.4807301461696625\n","Epoch: 19 Test Loss:  0.42843955755233765\n","Epoch: 19 Test Loss:  0.46474510431289673\n","Epoch: 19 Test Loss:  0.37971895933151245\n","Epoch: 19 Test Loss:  0.47564181685447693\n","Epoch: 20 Training Loss:  0.3922111392021179\n","Epoch: 20 Training Loss:  0.5148643255233765\n","Epoch: 20 Training Loss:  0.362373411655426\n","Epoch: 20 Training Loss:  0.42786845564842224\n","Epoch: 20 Training Loss:  0.4195229113101959\n","Epoch: 20 Training Loss:  0.34590497612953186\n","Epoch: 20 Training Loss:  0.38483625650405884\n","Epoch: 20 Training Loss:  0.45342594385147095\n","Epoch: 20 Training Loss:  0.45700564980506897\n","Epoch: 20 Training Loss:  0.45152005553245544\n","Epoch: 20 Training Loss:  0.4558667540550232\n","Epoch: 20 Training Loss:  0.4727829694747925\n","Epoch: 20 Test Loss:  0.4807301461696625\n","Epoch: 20 Test Loss:  0.42843955755233765\n","Epoch: 20 Test Loss:  0.46474510431289673\n","Epoch: 20 Test Loss:  0.37971895933151245\n","Epoch: 20 Test Loss:  0.47564181685447693\n","Epoch: 21 Training Loss:  0.3922111392021179\n","Epoch: 21 Training Loss:  0.5148643255233765\n","Epoch: 21 Training Loss:  0.362373411655426\n","Epoch: 21 Training Loss:  0.42786845564842224\n","Epoch: 21 Training Loss:  0.4195229113101959\n","Epoch: 21 Training Loss:  0.34590497612953186\n","Epoch: 21 Training Loss:  0.38483625650405884\n","Epoch: 21 Training Loss:  0.45342594385147095\n","Epoch: 21 Training Loss:  0.45700564980506897\n","Epoch: 21 Training Loss:  0.45152005553245544\n","Epoch: 21 Training Loss:  0.4558667540550232\n","Epoch: 21 Training Loss:  0.4727829694747925\n","Epoch: 21 Test Loss:  0.4807301461696625\n","Epoch: 21 Test Loss:  0.42843955755233765\n","Epoch: 21 Test Loss:  0.46474510431289673\n","Epoch: 21 Test Loss:  0.37971895933151245\n","Epoch: 21 Test Loss:  0.47564181685447693\n","Epoch: 22 Training Loss:  0.3922111392021179\n","Epoch: 22 Training Loss:  0.5148643255233765\n","Epoch: 22 Training Loss:  0.362373411655426\n","Epoch: 22 Training Loss:  0.42786845564842224\n","Epoch: 22 Training Loss:  0.4195229113101959\n","Epoch: 22 Training Loss:  0.34590497612953186\n","Epoch: 22 Training Loss:  0.38483625650405884\n","Epoch: 22 Training Loss:  0.45342594385147095\n","Epoch: 22 Training Loss:  0.45700564980506897\n","Epoch: 22 Training Loss:  0.45152005553245544\n","Epoch: 22 Training Loss:  0.4558667540550232\n","Epoch: 22 Training Loss:  0.4727829694747925\n","Epoch: 22 Test Loss:  0.4807301461696625\n","Epoch: 22 Test Loss:  0.42843955755233765\n","Epoch: 22 Test Loss:  0.46474510431289673\n","Epoch: 22 Test Loss:  0.37971895933151245\n","Epoch: 22 Test Loss:  0.47564181685447693\n","Epoch: 23 Training Loss:  0.3922111392021179\n","Epoch: 23 Training Loss:  0.5148643255233765\n","Epoch: 23 Training Loss:  0.362373411655426\n","Epoch: 23 Training Loss:  0.42786845564842224\n","Epoch: 23 Training Loss:  0.4195229113101959\n","Epoch: 23 Training Loss:  0.34590497612953186\n","Epoch: 23 Training Loss:  0.38483625650405884\n","Epoch: 23 Training Loss:  0.45342594385147095\n","Epoch: 23 Training Loss:  0.45700564980506897\n","Epoch: 23 Training Loss:  0.45152005553245544\n","Epoch: 23 Training Loss:  0.4558667540550232\n","Epoch: 23 Training Loss:  0.4727829694747925\n","Epoch: 23 Test Loss:  0.4807301461696625\n","Epoch: 23 Test Loss:  0.42843955755233765\n","Epoch: 23 Test Loss:  0.46474510431289673\n","Epoch: 23 Test Loss:  0.37971895933151245\n","Epoch: 23 Test Loss:  0.47564181685447693\n","Epoch: 24 Training Loss:  0.3922111392021179\n","Epoch: 24 Training Loss:  0.5148643255233765\n","Epoch: 24 Training Loss:  0.362373411655426\n","Epoch: 24 Training Loss:  0.42786845564842224\n","Epoch: 24 Training Loss:  0.4195229113101959\n","Epoch: 24 Training Loss:  0.34590497612953186\n","Epoch: 24 Training Loss:  0.38483625650405884\n","Epoch: 24 Training Loss:  0.45342594385147095\n","Epoch: 24 Training Loss:  0.45700564980506897\n","Epoch: 24 Training Loss:  0.45152005553245544\n","Epoch: 24 Training Loss:  0.4558667540550232\n","Epoch: 24 Training Loss:  0.4727829694747925\n","Epoch: 24 Test Loss:  0.4807301461696625\n","Epoch: 24 Test Loss:  0.42843955755233765\n","Epoch: 24 Test Loss:  0.46474510431289673\n","Epoch: 24 Test Loss:  0.37971895933151245\n","Epoch: 24 Test Loss:  0.47564181685447693\n","Epoch: 25 Training Loss:  0.3922111392021179\n","Epoch: 25 Training Loss:  0.5148643255233765\n","Epoch: 25 Training Loss:  0.362373411655426\n","Epoch: 25 Training Loss:  0.42786845564842224\n","Epoch: 25 Training Loss:  0.4195229113101959\n","Epoch: 25 Training Loss:  0.34590497612953186\n","Epoch: 25 Training Loss:  0.38483625650405884\n","Epoch: 25 Training Loss:  0.45342594385147095\n","Epoch: 25 Training Loss:  0.45700564980506897\n","Epoch: 25 Training Loss:  0.45152005553245544\n","Epoch: 25 Training Loss:  0.4558667540550232\n","Epoch: 25 Training Loss:  0.4727829694747925\n","Epoch: 25 Test Loss:  0.4807301461696625\n","Epoch: 25 Test Loss:  0.42843955755233765\n","Epoch: 25 Test Loss:  0.46474510431289673\n","Epoch: 25 Test Loss:  0.37971895933151245\n","Epoch: 25 Test Loss:  0.47564181685447693\n","Epoch: 26 Training Loss:  0.3922111392021179\n","Epoch: 26 Training Loss:  0.5148643255233765\n","Epoch: 26 Training Loss:  0.362373411655426\n","Epoch: 26 Training Loss:  0.42786845564842224\n","Epoch: 26 Training Loss:  0.4195229113101959\n","Epoch: 26 Training Loss:  0.34590497612953186\n","Epoch: 26 Training Loss:  0.38483625650405884\n","Epoch: 26 Training Loss:  0.45342594385147095\n","Epoch: 26 Training Loss:  0.45700564980506897\n","Epoch: 26 Training Loss:  0.45152005553245544\n","Epoch: 26 Training Loss:  0.4558667540550232\n","Epoch: 26 Training Loss:  0.4727829694747925\n","Epoch: 26 Test Loss:  0.4807301461696625\n","Epoch: 26 Test Loss:  0.42843955755233765\n","Epoch: 26 Test Loss:  0.46474510431289673\n","Epoch: 26 Test Loss:  0.37971895933151245\n","Epoch: 26 Test Loss:  0.47564181685447693\n","Epoch: 27 Training Loss:  0.3922111392021179\n","Epoch: 27 Training Loss:  0.5148643255233765\n","Epoch: 27 Training Loss:  0.362373411655426\n","Epoch: 27 Training Loss:  0.42786845564842224\n","Epoch: 27 Training Loss:  0.4195229113101959\n","Epoch: 27 Training Loss:  0.34590497612953186\n","Epoch: 27 Training Loss:  0.38483625650405884\n","Epoch: 27 Training Loss:  0.45342594385147095\n","Epoch: 27 Training Loss:  0.45700564980506897\n","Epoch: 27 Training Loss:  0.45152005553245544\n","Epoch: 27 Training Loss:  0.4558667540550232\n","Epoch: 27 Training Loss:  0.4727829694747925\n","Epoch: 27 Test Loss:  0.4807301461696625\n","Epoch: 27 Test Loss:  0.42843955755233765\n","Epoch: 27 Test Loss:  0.46474510431289673\n","Epoch: 27 Test Loss:  0.37971895933151245\n","Epoch: 27 Test Loss:  0.47564181685447693\n","Epoch: 28 Training Loss:  0.3922111392021179\n","Epoch: 28 Training Loss:  0.5148643255233765\n","Epoch: 28 Training Loss:  0.362373411655426\n","Epoch: 28 Training Loss:  0.42786845564842224\n","Epoch: 28 Training Loss:  0.4195229113101959\n","Epoch: 28 Training Loss:  0.34590497612953186\n","Epoch: 28 Training Loss:  0.38483625650405884\n","Epoch: 28 Training Loss:  0.45342594385147095\n","Epoch: 28 Training Loss:  0.45700564980506897\n","Epoch: 28 Training Loss:  0.45152005553245544\n","Epoch: 28 Training Loss:  0.4558667540550232\n","Epoch: 28 Training Loss:  0.4727829694747925\n","Epoch: 28 Test Loss:  0.4807301461696625\n","Epoch: 28 Test Loss:  0.42843955755233765\n","Epoch: 28 Test Loss:  0.46474510431289673\n","Epoch: 28 Test Loss:  0.37971895933151245\n","Epoch: 28 Test Loss:  0.47564181685447693\n","Epoch: 29 Training Loss:  0.3922111392021179\n","Epoch: 29 Training Loss:  0.5148643255233765\n","Epoch: 29 Training Loss:  0.362373411655426\n","Epoch: 29 Training Loss:  0.42786845564842224\n","Epoch: 29 Training Loss:  0.4195229113101959\n","Epoch: 29 Training Loss:  0.34590497612953186\n","Epoch: 29 Training Loss:  0.38483625650405884\n","Epoch: 29 Training Loss:  0.45342594385147095\n","Epoch: 29 Training Loss:  0.45700564980506897\n","Epoch: 29 Training Loss:  0.45152005553245544\n","Epoch: 29 Training Loss:  0.4558667540550232\n","Epoch: 29 Training Loss:  0.4727829694747925\n","Epoch: 29 Test Loss:  0.4807301461696625\n","Epoch: 29 Test Loss:  0.42843955755233765\n","Epoch: 29 Test Loss:  0.46474510431289673\n","Epoch: 29 Test Loss:  0.37971895933151245\n","Epoch: 29 Test Loss:  0.47564181685447693\n","Epoch: 30 Training Loss:  0.3922111392021179\n","Epoch: 30 Training Loss:  0.5148643255233765\n","Epoch: 30 Training Loss:  0.362373411655426\n","Epoch: 30 Training Loss:  0.42786845564842224\n","Epoch: 30 Training Loss:  0.4195229113101959\n","Epoch: 30 Training Loss:  0.34590497612953186\n","Epoch: 30 Training Loss:  0.38483625650405884\n","Epoch: 30 Training Loss:  0.45342594385147095\n","Epoch: 30 Training Loss:  0.45700564980506897\n","Epoch: 30 Training Loss:  0.45152005553245544\n","Epoch: 30 Training Loss:  0.4558667540550232\n","Epoch: 30 Training Loss:  0.4727829694747925\n","Epoch: 30 Test Loss:  0.4807301461696625\n","Epoch: 30 Test Loss:  0.42843955755233765\n","Epoch: 30 Test Loss:  0.46474510431289673\n","Epoch: 30 Test Loss:  0.37971895933151245\n","Epoch: 30 Test Loss:  0.47564181685447693\n","Epoch: 31 Training Loss:  0.3922111392021179\n","Epoch: 31 Training Loss:  0.5148643255233765\n","Epoch: 31 Training Loss:  0.362373411655426\n","Epoch: 31 Training Loss:  0.42786845564842224\n","Epoch: 31 Training Loss:  0.4195229113101959\n","Epoch: 31 Training Loss:  0.34590497612953186\n","Epoch: 31 Training Loss:  0.38483625650405884\n","Epoch: 31 Training Loss:  0.45342594385147095\n","Epoch: 31 Training Loss:  0.45700564980506897\n","Epoch: 31 Training Loss:  0.45152005553245544\n","Epoch: 31 Training Loss:  0.4558667540550232\n","Epoch: 31 Training Loss:  0.4727829694747925\n","Epoch: 31 Test Loss:  0.4807301461696625\n","Epoch: 31 Test Loss:  0.42843955755233765\n","Epoch: 31 Test Loss:  0.46474510431289673\n","Epoch: 31 Test Loss:  0.37971895933151245\n","Epoch: 31 Test Loss:  0.47564181685447693\n","Epoch: 32 Training Loss:  0.3922111392021179\n","Epoch: 32 Training Loss:  0.5148643255233765\n","Epoch: 32 Training Loss:  0.362373411655426\n","Epoch: 32 Training Loss:  0.42786845564842224\n","Epoch: 32 Training Loss:  0.4195229113101959\n","Epoch: 32 Training Loss:  0.34590497612953186\n","Epoch: 32 Training Loss:  0.38483625650405884\n","Epoch: 32 Training Loss:  0.45342594385147095\n","Epoch: 32 Training Loss:  0.45700564980506897\n","Epoch: 32 Training Loss:  0.45152005553245544\n","Epoch: 32 Training Loss:  0.4558667540550232\n","Epoch: 32 Training Loss:  0.4727829694747925\n","Epoch: 32 Test Loss:  0.4807301461696625\n","Epoch: 32 Test Loss:  0.42843955755233765\n","Epoch: 32 Test Loss:  0.46474510431289673\n","Epoch: 32 Test Loss:  0.37971895933151245\n","Epoch: 32 Test Loss:  0.47564181685447693\n","Epoch: 33 Training Loss:  0.3922111392021179\n","Epoch: 33 Training Loss:  0.5148643255233765\n","Epoch: 33 Training Loss:  0.362373411655426\n","Epoch: 33 Training Loss:  0.42786845564842224\n","Epoch: 33 Training Loss:  0.4195229113101959\n","Epoch: 33 Training Loss:  0.34590497612953186\n","Epoch: 33 Training Loss:  0.38483625650405884\n","Epoch: 33 Training Loss:  0.45342594385147095\n","Epoch: 33 Training Loss:  0.45700564980506897\n","Epoch: 33 Training Loss:  0.45152005553245544\n","Epoch: 33 Training Loss:  0.4558667540550232\n","Epoch: 33 Training Loss:  0.4727829694747925\n","Epoch: 33 Test Loss:  0.4807301461696625\n","Epoch: 33 Test Loss:  0.42843955755233765\n","Epoch: 33 Test Loss:  0.46474510431289673\n","Epoch: 33 Test Loss:  0.37971895933151245\n","Epoch: 33 Test Loss:  0.47564181685447693\n","Epoch: 34 Training Loss:  0.3922111392021179\n","Epoch: 34 Training Loss:  0.5148643255233765\n","Epoch: 34 Training Loss:  0.362373411655426\n","Epoch: 34 Training Loss:  0.42786845564842224\n","Epoch: 34 Training Loss:  0.4195229113101959\n","Epoch: 34 Training Loss:  0.34590497612953186\n","Epoch: 34 Training Loss:  0.38483625650405884\n","Epoch: 34 Training Loss:  0.45342594385147095\n","Epoch: 34 Training Loss:  0.45700564980506897\n","Epoch: 34 Training Loss:  0.45152005553245544\n","Epoch: 34 Training Loss:  0.4558667540550232\n","Epoch: 34 Training Loss:  0.4727829694747925\n","Epoch: 34 Test Loss:  0.4807301461696625\n","Epoch: 34 Test Loss:  0.42843955755233765\n","Epoch: 34 Test Loss:  0.46474510431289673\n","Epoch: 34 Test Loss:  0.37971895933151245\n","Epoch: 34 Test Loss:  0.47564181685447693\n","Epoch: 35 Training Loss:  0.3922111392021179\n","Epoch: 35 Training Loss:  0.5148643255233765\n","Epoch: 35 Training Loss:  0.362373411655426\n","Epoch: 35 Training Loss:  0.42786845564842224\n","Epoch: 35 Training Loss:  0.4195229113101959\n","Epoch: 35 Training Loss:  0.34590497612953186\n","Epoch: 35 Training Loss:  0.38483625650405884\n","Epoch: 35 Training Loss:  0.45342594385147095\n","Epoch: 35 Training Loss:  0.45700564980506897\n","Epoch: 35 Training Loss:  0.45152005553245544\n","Epoch: 35 Training Loss:  0.4558667540550232\n","Epoch: 35 Training Loss:  0.4727829694747925\n","Epoch: 35 Test Loss:  0.4807301461696625\n","Epoch: 35 Test Loss:  0.42843955755233765\n","Epoch: 35 Test Loss:  0.46474510431289673\n","Epoch: 35 Test Loss:  0.37971895933151245\n","Epoch: 35 Test Loss:  0.47564181685447693\n","Epoch: 36 Training Loss:  0.3922111392021179\n","Epoch: 36 Training Loss:  0.5148643255233765\n","Epoch: 36 Training Loss:  0.362373411655426\n","Epoch: 36 Training Loss:  0.42786845564842224\n","Epoch: 36 Training Loss:  0.4195229113101959\n","Epoch: 36 Training Loss:  0.34590497612953186\n","Epoch: 36 Training Loss:  0.38483625650405884\n","Epoch: 36 Training Loss:  0.45342594385147095\n","Epoch: 36 Training Loss:  0.45700564980506897\n","Epoch: 36 Training Loss:  0.45152005553245544\n","Epoch: 36 Training Loss:  0.4558667540550232\n","Epoch: 36 Training Loss:  0.4727829694747925\n","Epoch: 36 Test Loss:  0.4807301461696625\n","Epoch: 36 Test Loss:  0.42843955755233765\n","Epoch: 36 Test Loss:  0.46474510431289673\n","Epoch: 36 Test Loss:  0.37971895933151245\n","Epoch: 36 Test Loss:  0.47564181685447693\n","Epoch: 37 Training Loss:  0.3922111392021179\n","Epoch: 37 Training Loss:  0.5148643255233765\n","Epoch: 37 Training Loss:  0.362373411655426\n","Epoch: 37 Training Loss:  0.42786845564842224\n","Epoch: 37 Training Loss:  0.4195229113101959\n","Epoch: 37 Training Loss:  0.34590497612953186\n","Epoch: 37 Training Loss:  0.38483625650405884\n","Epoch: 37 Training Loss:  0.45342594385147095\n","Epoch: 37 Training Loss:  0.45700564980506897\n","Epoch: 37 Training Loss:  0.45152005553245544\n","Epoch: 37 Training Loss:  0.4558667540550232\n","Epoch: 37 Training Loss:  0.4727829694747925\n","Epoch: 37 Test Loss:  0.4807301461696625\n","Epoch: 37 Test Loss:  0.42843955755233765\n","Epoch: 37 Test Loss:  0.46474510431289673\n","Epoch: 37 Test Loss:  0.37971895933151245\n","Epoch: 37 Test Loss:  0.47564181685447693\n","Epoch: 38 Training Loss:  0.3922111392021179\n","Epoch: 38 Training Loss:  0.5148643255233765\n","Epoch: 38 Training Loss:  0.362373411655426\n","Epoch: 38 Training Loss:  0.42786845564842224\n","Epoch: 38 Training Loss:  0.4195229113101959\n","Epoch: 38 Training Loss:  0.34590497612953186\n","Epoch: 38 Training Loss:  0.38483625650405884\n","Epoch: 38 Training Loss:  0.45342594385147095\n","Epoch: 38 Training Loss:  0.45700564980506897\n","Epoch: 38 Training Loss:  0.45152005553245544\n","Epoch: 38 Training Loss:  0.4558667540550232\n","Epoch: 38 Training Loss:  0.4727829694747925\n","Epoch: 38 Test Loss:  0.4807301461696625\n","Epoch: 38 Test Loss:  0.42843955755233765\n","Epoch: 38 Test Loss:  0.46474510431289673\n","Epoch: 38 Test Loss:  0.37971895933151245\n","Epoch: 38 Test Loss:  0.47564181685447693\n","Epoch: 39 Training Loss:  0.3922111392021179\n","Epoch: 39 Training Loss:  0.5148643255233765\n","Epoch: 39 Training Loss:  0.362373411655426\n","Epoch: 39 Training Loss:  0.42786845564842224\n","Epoch: 39 Training Loss:  0.4195229113101959\n","Epoch: 39 Training Loss:  0.34590497612953186\n","Epoch: 39 Training Loss:  0.38483625650405884\n","Epoch: 39 Training Loss:  0.45342594385147095\n","Epoch: 39 Training Loss:  0.45700564980506897\n","Epoch: 39 Training Loss:  0.45152005553245544\n","Epoch: 39 Training Loss:  0.4558667540550232\n","Epoch: 39 Training Loss:  0.4727829694747925\n","Epoch: 39 Test Loss:  0.4807301461696625\n","Epoch: 39 Test Loss:  0.42843955755233765\n","Epoch: 39 Test Loss:  0.46474510431289673\n","Epoch: 39 Test Loss:  0.37971895933151245\n","Epoch: 39 Test Loss:  0.47564181685447693\n","Epoch: 40 Training Loss:  0.3922111392021179\n","Epoch: 40 Training Loss:  0.5148643255233765\n","Epoch: 40 Training Loss:  0.362373411655426\n","Epoch: 40 Training Loss:  0.42786845564842224\n","Epoch: 40 Training Loss:  0.4195229113101959\n","Epoch: 40 Training Loss:  0.34590497612953186\n","Epoch: 40 Training Loss:  0.38483625650405884\n","Epoch: 40 Training Loss:  0.45342594385147095\n","Epoch: 40 Training Loss:  0.45700564980506897\n","Epoch: 40 Training Loss:  0.45152005553245544\n","Epoch: 40 Training Loss:  0.4558667540550232\n","Epoch: 40 Training Loss:  0.4727829694747925\n","Epoch: 40 Test Loss:  0.4807301461696625\n","Epoch: 40 Test Loss:  0.42843955755233765\n","Epoch: 40 Test Loss:  0.46474510431289673\n","Epoch: 40 Test Loss:  0.37971895933151245\n","Epoch: 40 Test Loss:  0.47564181685447693\n","Epoch: 41 Training Loss:  0.3922111392021179\n","Epoch: 41 Training Loss:  0.5148643255233765\n","Epoch: 41 Training Loss:  0.362373411655426\n","Epoch: 41 Training Loss:  0.42786845564842224\n","Epoch: 41 Training Loss:  0.4195229113101959\n","Epoch: 41 Training Loss:  0.34590497612953186\n","Epoch: 41 Training Loss:  0.38483625650405884\n","Epoch: 41 Training Loss:  0.45342594385147095\n","Epoch: 41 Training Loss:  0.45700564980506897\n","Epoch: 41 Training Loss:  0.45152005553245544\n","Epoch: 41 Training Loss:  0.4558667540550232\n","Epoch: 41 Training Loss:  0.4727829694747925\n","Epoch: 41 Test Loss:  0.4807301461696625\n","Epoch: 41 Test Loss:  0.42843955755233765\n","Epoch: 41 Test Loss:  0.46474510431289673\n","Epoch: 41 Test Loss:  0.37971895933151245\n","Epoch: 41 Test Loss:  0.47564181685447693\n","Epoch: 42 Training Loss:  0.3922111392021179\n","Epoch: 42 Training Loss:  0.5148643255233765\n","Epoch: 42 Training Loss:  0.362373411655426\n","Epoch: 42 Training Loss:  0.42786845564842224\n","Epoch: 42 Training Loss:  0.4195229113101959\n","Epoch: 42 Training Loss:  0.34590497612953186\n","Epoch: 42 Training Loss:  0.38483625650405884\n","Epoch: 42 Training Loss:  0.45342594385147095\n","Epoch: 42 Training Loss:  0.45700564980506897\n","Epoch: 42 Training Loss:  0.45152005553245544\n","Epoch: 42 Training Loss:  0.4558667540550232\n","Epoch: 42 Training Loss:  0.4727829694747925\n","Epoch: 42 Test Loss:  0.4807301461696625\n","Epoch: 42 Test Loss:  0.42843955755233765\n","Epoch: 42 Test Loss:  0.46474510431289673\n","Epoch: 42 Test Loss:  0.37971895933151245\n","Epoch: 42 Test Loss:  0.47564181685447693\n","Epoch: 43 Training Loss:  0.3922111392021179\n","Epoch: 43 Training Loss:  0.5148643255233765\n","Epoch: 43 Training Loss:  0.362373411655426\n","Epoch: 43 Training Loss:  0.42786845564842224\n","Epoch: 43 Training Loss:  0.4195229113101959\n","Epoch: 43 Training Loss:  0.34590497612953186\n","Epoch: 43 Training Loss:  0.38483625650405884\n","Epoch: 43 Training Loss:  0.45342594385147095\n","Epoch: 43 Training Loss:  0.45700564980506897\n","Epoch: 43 Training Loss:  0.45152005553245544\n","Epoch: 43 Training Loss:  0.4558667540550232\n","Epoch: 43 Training Loss:  0.4727829694747925\n","Epoch: 43 Test Loss:  0.4807301461696625\n","Epoch: 43 Test Loss:  0.42843955755233765\n","Epoch: 43 Test Loss:  0.46474510431289673\n","Epoch: 43 Test Loss:  0.37971895933151245\n","Epoch: 43 Test Loss:  0.47564181685447693\n","Epoch: 44 Training Loss:  0.3922111392021179\n","Epoch: 44 Training Loss:  0.5148643255233765\n","Epoch: 44 Training Loss:  0.362373411655426\n","Epoch: 44 Training Loss:  0.42786845564842224\n","Epoch: 44 Training Loss:  0.4195229113101959\n","Epoch: 44 Training Loss:  0.34590497612953186\n","Epoch: 44 Training Loss:  0.38483625650405884\n","Epoch: 44 Training Loss:  0.45342594385147095\n","Epoch: 44 Training Loss:  0.45700564980506897\n","Epoch: 44 Training Loss:  0.45152005553245544\n","Epoch: 44 Training Loss:  0.4558667540550232\n","Epoch: 44 Training Loss:  0.4727829694747925\n","Epoch: 44 Test Loss:  0.4807301461696625\n","Epoch: 44 Test Loss:  0.42843955755233765\n","Epoch: 44 Test Loss:  0.46474510431289673\n","Epoch: 44 Test Loss:  0.37971895933151245\n","Epoch: 44 Test Loss:  0.47564181685447693\n","Epoch: 45 Training Loss:  0.3922111392021179\n","Epoch: 45 Training Loss:  0.5148643255233765\n","Epoch: 45 Training Loss:  0.362373411655426\n","Epoch: 45 Training Loss:  0.42786845564842224\n","Epoch: 45 Training Loss:  0.4195229113101959\n","Epoch: 45 Training Loss:  0.34590497612953186\n","Epoch: 45 Training Loss:  0.38483625650405884\n","Epoch: 45 Training Loss:  0.45342594385147095\n","Epoch: 45 Training Loss:  0.45700564980506897\n","Epoch: 45 Training Loss:  0.45152005553245544\n","Epoch: 45 Training Loss:  0.4558667540550232\n","Epoch: 45 Training Loss:  0.4727829694747925\n","Epoch: 45 Test Loss:  0.4807301461696625\n","Epoch: 45 Test Loss:  0.42843955755233765\n","Epoch: 45 Test Loss:  0.46474510431289673\n","Epoch: 45 Test Loss:  0.37971895933151245\n","Epoch: 45 Test Loss:  0.47564181685447693\n","Epoch: 46 Training Loss:  0.3922111392021179\n","Epoch: 46 Training Loss:  0.5148643255233765\n","Epoch: 46 Training Loss:  0.362373411655426\n","Epoch: 46 Training Loss:  0.42786845564842224\n","Epoch: 46 Training Loss:  0.4195229113101959\n","Epoch: 46 Training Loss:  0.34590497612953186\n","Epoch: 46 Training Loss:  0.38483625650405884\n","Epoch: 46 Training Loss:  0.45342594385147095\n","Epoch: 46 Training Loss:  0.45700564980506897\n","Epoch: 46 Training Loss:  0.45152005553245544\n","Epoch: 46 Training Loss:  0.4558667540550232\n","Epoch: 46 Training Loss:  0.4727829694747925\n","Epoch: 46 Test Loss:  0.4807301461696625\n","Epoch: 46 Test Loss:  0.42843955755233765\n","Epoch: 46 Test Loss:  0.46474510431289673\n","Epoch: 46 Test Loss:  0.37971895933151245\n","Epoch: 46 Test Loss:  0.47564181685447693\n","Epoch: 47 Training Loss:  0.3922111392021179\n","Epoch: 47 Training Loss:  0.5148643255233765\n","Epoch: 47 Training Loss:  0.362373411655426\n","Epoch: 47 Training Loss:  0.42786845564842224\n","Epoch: 47 Training Loss:  0.4195229113101959\n","Epoch: 47 Training Loss:  0.34590497612953186\n","Epoch: 47 Training Loss:  0.38483625650405884\n","Epoch: 47 Training Loss:  0.45342594385147095\n","Epoch: 47 Training Loss:  0.45700564980506897\n","Epoch: 47 Training Loss:  0.45152005553245544\n","Epoch: 47 Training Loss:  0.4558667540550232\n","Epoch: 47 Training Loss:  0.4727829694747925\n","Epoch: 47 Test Loss:  0.4807301461696625\n","Epoch: 47 Test Loss:  0.42843955755233765\n","Epoch: 47 Test Loss:  0.46474510431289673\n","Epoch: 47 Test Loss:  0.37971895933151245\n","Epoch: 47 Test Loss:  0.47564181685447693\n","Epoch: 48 Training Loss:  0.3922111392021179\n","Epoch: 48 Training Loss:  0.5148643255233765\n","Epoch: 48 Training Loss:  0.362373411655426\n","Epoch: 48 Training Loss:  0.42786845564842224\n","Epoch: 48 Training Loss:  0.4195229113101959\n","Epoch: 48 Training Loss:  0.34590497612953186\n","Epoch: 48 Training Loss:  0.38483625650405884\n","Epoch: 48 Training Loss:  0.45342594385147095\n","Epoch: 48 Training Loss:  0.45700564980506897\n","Epoch: 48 Training Loss:  0.45152005553245544\n","Epoch: 48 Training Loss:  0.4558667540550232\n","Epoch: 48 Training Loss:  0.4727829694747925\n","Epoch: 48 Test Loss:  0.4807301461696625\n","Epoch: 48 Test Loss:  0.42843955755233765\n","Epoch: 48 Test Loss:  0.46474510431289673\n","Epoch: 48 Test Loss:  0.37971895933151245\n","Epoch: 48 Test Loss:  0.47564181685447693\n","Epoch: 49 Training Loss:  0.3922111392021179\n","Epoch: 49 Training Loss:  0.5148643255233765\n","Epoch: 49 Training Loss:  0.362373411655426\n","Epoch: 49 Training Loss:  0.42786845564842224\n","Epoch: 49 Training Loss:  0.4195229113101959\n","Epoch: 49 Training Loss:  0.34590497612953186\n","Epoch: 49 Training Loss:  0.38483625650405884\n","Epoch: 49 Training Loss:  0.45342594385147095\n","Epoch: 49 Training Loss:  0.45700564980506897\n","Epoch: 49 Training Loss:  0.45152005553245544\n","Epoch: 49 Training Loss:  0.4558667540550232\n","Epoch: 49 Training Loss:  0.4727829694747925\n","Epoch: 49 Test Loss:  0.4807301461696625\n","Epoch: 49 Test Loss:  0.42843955755233765\n","Epoch: 49 Test Loss:  0.46474510431289673\n","Epoch: 49 Test Loss:  0.37971895933151245\n","Epoch: 49 Test Loss:  0.47564181685447693\n","Epoch: 50 Training Loss:  0.3922111392021179\n","Epoch: 50 Training Loss:  0.5148643255233765\n","Epoch: 50 Training Loss:  0.362373411655426\n","Epoch: 50 Training Loss:  0.42786845564842224\n","Epoch: 50 Training Loss:  0.4195229113101959\n","Epoch: 50 Training Loss:  0.34590497612953186\n","Epoch: 50 Training Loss:  0.38483625650405884\n","Epoch: 50 Training Loss:  0.45342594385147095\n","Epoch: 50 Training Loss:  0.45700564980506897\n","Epoch: 50 Training Loss:  0.45152005553245544\n","Epoch: 50 Training Loss:  0.4558667540550232\n","Epoch: 50 Training Loss:  0.4727829694747925\n","Epoch: 50 Test Loss:  0.4807301461696625\n","Epoch: 50 Test Loss:  0.42843955755233765\n","Epoch: 50 Test Loss:  0.46474510431289673\n","Epoch: 50 Test Loss:  0.37971895933151245\n","Epoch: 50 Test Loss:  0.47564181685447693\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7uF9GmHKU2tq"},"source":["[iuoiuo=p[]t]"],"execution_count":null,"outputs":[]}]}
